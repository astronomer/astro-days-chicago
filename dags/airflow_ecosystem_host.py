"""
airflow_ecosystem_host
DAG auto-generated by Astro Build.
"""

from airflow.decorators import dag
from astro import sql as aql
from astro.sql.table import Table
import pandas as pd
import pendulum


@aql.transform(conn_id="big_query", task_id="pypi_downloads")
def pypi_downloads_func():
    return """select
   package_name,
   sum(num_downloads) downloads,
   extract(year from date) as year,
   extract(month from date) as month
from pypi_data.downloads
group by package_name, year, month
order by year, month desc;"""

@aql.transform(conn_id="snowflake", task_id="github_issues")
def github_issues_func():
    return """select
   year(created_ts) year,
   month(created_ts) month,
   author_type,
   count(*) count
from github.airflow.issues
group by year(created_ts), month(created_ts), author_type;"""

@aql.dataframe(task_id="transform_downloads")
def transform_downloads_func(pypi_downloads: pd.DataFrame):
    transformed_df = pypi_downloads
     
    # rename the downloads column to be more explicit
    transformed_df.rename(
       columns = { 'downloads': 'pip_downloads' },
       inplace = True  
    )
     
    # drop the package_name column, we don't need it
    transformed_df.drop(columns = ['package_name'], inplace = True)
     
    return transformed_df

@aql.dataframe(task_id="transform_gh_issues")
def transform_gh_issues_func(github_issues: pd.DataFrame):
    transformed_df = github_issues
     
    # fill the null values with "other" bc we don't know what the author type is
    transformed_df['author_type'] = transformed_df['author_type'].fillna('other')
     
    # pivot from long form to wide form
    pivoted_df = transformed_df.pivot(
      index = ['year', 'month'],
      columns = 'author_type',
      values = 'count'
    ).reset_index().fillna(0)
     
    # rename columns to be clearer
    pivoted_df.rename(
      columns = {
          'collaborator': 'collaborator_gh_issues',
          'contributor': 'contributor_gh_issues',
          'member': 'member_gh_issues',
          'other': 'other_gh_issues'
      },
      inplace = True
    )
     
    # add a column to sum all the gh issues
    pivoted_df['all_gh_issues'] = pivoted_df[[
      col for col in pivoted_df.columns
      if col.endswith('gh_issues')
    ]].sum(axis=1)
     
    return pivoted_df.sort_values(['year', 'month'])

@aql.run_raw_sql(conn_id="snowflake", task_id="create_table")
def create_table_func(aggregate: Table):
    return """create or replace table sandbox.jl_out.ecosystem_data
as select * from {{aggregate}} where year >= 2016;"""

@aql.dataframe(task_id="aggregate")
def aggregate_func(transform_downloads: pd.DataFrame, transform_gh_issues: pd.DataFrame):
    # join the github issues and pypi downloads
    return transform_gh_issues.merge(
      transform_downloads,
      how = 'outer',
      on = ['year', 'month']
    ).fillna(value = 0)

@dag(
    schedule_interval=None,
    start_date=pendulum.from_format("2022-12-06", "YYYY-MM-DD"),
)
def airflow_ecosystem_host():
    pypi_downloads = pypi_downloads_func()

    transform_downloads = transform_downloads_func(
        pypi_downloads,
    )

    github_issues = github_issues_func()

    transform_gh_issues = transform_gh_issues_func(
        github_issues,
    )

    aggregate = aggregate_func(
        transform_downloads, transform_gh_issues,
    )

    create_table = create_table_func(
        aggregate,)

    aggregate << [transform_downloads, transform_gh_issues]

    create_table << aggregate

    transform_downloads << pypi_downloads

    transform_gh_issues << github_issues

dag_obj = airflow_ecosystem_host()
